{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Abstract"},{"metadata":{},"cell_type":"markdown","source":"In this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nSubmission File: For every row in the test.csv, submission files should contain two columns: ID and target. The ID corresponds to the column of that ID in the test.csv.\nFor the prediction of the Target column used LightGBM and XGboost and at the end combined the predictions of both the libraries and generated the sub_lgb_xgb.csv file as the output."},{"metadata":{},"cell_type":"markdown","source":"Importing the libraries of pandas and lightgbm. Loading the train.csv and test.csv files."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Retreiving the first five rows of train dataframe"},{"metadata":{},"cell_type":"markdown","source":"![](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Retreiving the first five rows of test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the NaN values from the loades data in the train dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Train Features with NaN Values = \" + str(train.columns[train.isnull().sum() != 0].size))\nif (train.columns[train.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train.columns[train.isnull().sum() != 0])))\n    train[train.columns[train.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the NaN values from the loades data in the test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Test Features with NaN Values = \" + str(test.columns[test.isnull().sum() != 0].size))\nif (test.columns[test.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test.columns[test.isnull().sum() != 0])))\n    test[test.columns[test.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing numpy library and dropping target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nX_train = train.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train[\"target\"].values)\n\nX_test = test.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing constant columns in the training set and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\nX_test.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the duplicate columns in the training set and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X_train.columns\n\nfor i in range(len(columns)-1):\n    v = X_train[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X_train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\nX_test.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\nprint(dupList)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingRegressor\n# clf_gb = GradientBoostingRegressor(random_state = 42)\n# clf_gb.fit(X_train, y_train)\n# print(clf_gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfeat_importances = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances = feat_importances.nlargest(100)\nplt.figure(figsize=(16,15))\nfeat_importances.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# feat_importances_gb = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\n# feat_importances_gb = feat_importances_gb.nlargest(25)\n# plt.figure(figsize=(16,8))\n# feat_importances_gb.plot(kind='barh')\n# plt.gca().invert_yaxis()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# clf_rf = RandomForestRegressor(random_state = 42)\n# clf_rf.fit(X_train, y_train)\n# print(clf_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def evaluate(model, features, labels):\n#     predictions = model.predict(features)\n#     errors = abs(predictions - labels)\n#     m = 100 * np.mean(errors / labels)\n#     accuracy = 100 - m\n#     print('Model Performance')\n#     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n#     return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# from sklearn.preprocessing import OneHotEncoder\n# onehotencoder = OneHotEncoder()\n# for column in train.columns:\n#     train[column] = onehotencoder.fit_transform(train[column])\n#     train[column] = train[column].reshape(1, -1)\n#     print(train[column])\n    \n    \n    \n#     #print test       # Produces: [ 0.58  0.76]\n# #print test.shape # Produces: (2,) meaning 2 rows, 1 col\n\n# #test = test.reshape(1, -1)\n# #print test       # Produces: [[ 0.58  0.76]]\n# #print test.shape \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# RF_target=np.expm1(clf_rf.predict(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"feat_importances_rf = pd.Series(clf_rf.feature_importances_, index=X_train.columns)\nfeat_importances_rf = feat_importances_rf.nlargest(25)\nplt.figure(figsize=(16,8))\nfeat_importances_rf.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Impoting library and generating pairplot of Top 6 Important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndf_plot = X_train[['f190486d6', 'eeb9cd3aa', '58e2e02e6', '58232a6fb', '15ace8c9f', '9fd594eec']]\ndf_plot['target'] = y_train\n\ng = sns.pairplot(df_plot, diag_kind=\"kde\", palette=\"BuGn_r\")\ng.fig.suptitle('Pairplot of Top 6 Important Features',fontsize=26)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"s1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(20).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(20).index\n\ncommon_features = pd.Series(list(set(s1).union(set(s2)))).values\n\nprint(common_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generating correlation Heatmap."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plot = pd.DataFrame(X_train, columns = common_features)\ncorr = df_plot.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation HeatMap\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build Test and Train data for modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM for predicting the target value. Light GBM is a gradient boosting framework that uses tree based learning algorithm. Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 35,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=500, \n                      verbose_eval=50, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\n# print(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# # feature importance\n# print(\"Features Importance...\")\n# gain = model.feature_importance('gain')\n# featureimp = pd.DataFrame({'feature':model.feature_name(), \n#                    'split':model.feature_importance('split'), \n#                    'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n# print(featureimp[:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def run_xgb(train_X, train_y, val_X, val_y, test_X):\n#     print(\"############# Build XGBoost model #############\")\n#     model_xgb = xgb.XGBRegressor(colsample_bytree = 0.055, \n#                                  colsample_bylevel = 0.5, \n#                                  gamma = 1.5, \n#                                  learning_rate = 0.02, \n#                                  max_depth = 32, \n#                                  objective = 'reg:linear',\n#                                  booster = 'gbtree',\n#                                  min_child_weight = 57, \n#                                  n_estimators = 1000, \n#                                  reg_alpha = 0, \n#                                  reg_lambda = 0,\n#                                  eval_metric = 'rmse', \n#                                  subsample = 0.7, \n#                                  silent = 1, \n#                                  n_jobs = -1, \n#                                  early_stopping_rounds = 14,\n#                                  random_state = 7, \n#                                  nthread = -1)\n    \n#     print(\"Validating.....\")\n#     model_xgb.fit(x_train, y_train, eval_set = [(x_train, y_train), (x_val, y_val)], eval_metric = 'rmse', \n#                   early_stopping_rounds = 100, verbose = True)\n#     pred_test_y = np.expm1(model_xgb.predict(test_X))\n#     print(\"XGBoost Training Completed...\")\n    \n#     return pred_test_y\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# sub=pd.read_csv('../input/sample_submission.csv')\n\n# sub_rf = pd.DataFrame()\n# sub_rf[\"target\"] = RF_target\n# sub_rf[\"ID\"] = sub[\"ID\"]\n# sub_rf.to_csv(\"sub_rf.csv\", index=False)\n\n# sub_lgb = pd.DataFrame()\n# sub_lgb[\"target\"] = lgb_pred\n# sub_lgb[\"ID\"] = sub[\"ID\"]\n# sub_lgb.to_csv(\"sub_lgb.csv\", index=False)\n\n\n# sub[\"target\"] = (sub_lgb[\"target\"] + sub_rf['target'] )/2\n\n# print(sub.head())\n# sub.to_csv('sub_lgb_xgb.csv', index=False)\n# sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# # feature importance\n# print(\"Features Importance...\")\n# gain = model.feature_importance('gain')\n# featureimp = pd.DataFrame({'feature':model.feature_name(), \n#                    'split':model.feature_importance('split'), \n#                    'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n# print(featureimp[:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# def run_xgb(dev_X, dev_y, val_X, val_y, X_test):\n#     params = {'objective': 'reg:linear', \n#           'eval_metric': 'rmse',\n#           'eta': 0.001,\n#           'max_depth': 10, \n#           'subsample': 0.6, \n#           'colsample_bytree': 0.6,\n#           'alpha':0.001,\n#           'random_state': 42, \n#           'silent': True}\n    \n#     tr_data = xgb.DMatrix(train_X, train_y)\n#     va_data = xgb.DMatrix(val_X, val_y)\n    \n#     watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n#     model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n#     dtest = xgb.DMatrix(test_X)\n#     xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n#     return xgb_pred_y, model_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# pred_test, model, evals_result = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\n# print(\"xgboost Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB modeling to find the Target column. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ndef run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.005,\n          'max_depth': 15, \n          'subsample': 0.7, \n          'colsample_bytree': 0.5,\n          'alpha':0,\n          'random_state': 42, \n          'silent': True}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 30, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training XGB\npred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"XGB Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# sub = pd.read_csv('../input/sample_submission.csv')\n# sub[\"target\"] = pred_test\n# print(sub.head())\n# sub.to_csv('sub_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining predictions from both LightGBM and xgBoost model."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\n\nsub_lgb = pd.DataFrame()\nsub_lgb[\"target\"] = pred_test\n\nsub_xgb = pd.DataFrame()\nsub_xgb[\"target\"] = pred_test_xgb\n\nsub[\"target\"] = (sub_lgb[\"target\"] + sub_xgb[\"target\"])/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generating .CSV file for submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub.head())\nsub.to_csv('sub_lgb_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSION**"},{"metadata":{},"cell_type":"markdown","source":"From the above observations we can infer that:\n\n###### The XGboost classifier has a Mean F-Score of 0.344\n######  \n\nAs F-Score value is a measure of a test's accuracy, hence a model with high Mean F-score is preferred. \nFrom the above results, we prefer AdaBoost classifier as it has maximum F-Score value"},{"metadata":{},"cell_type":"markdown","source":"**CONTRIBUTION STATEMENT**"},{"metadata":{},"cell_type":"markdown","source":"60% of code from online resources 40% of code by me."},{"metadata":{},"cell_type":"markdown","source":"**CITATIONS**"},{"metadata":{},"cell_type":"markdown","source":"Kaggle: https://www.kaggle.com/ \nWikipedia: https://en.wikipedia.org/ \nStackOverflow: https://stackoverflow.com/\nGithub: https://github.com/nikbearbrown/INFO_6105\nclassroom.github.com/assignment-invitations/a1008a519017ca80da27fb845ab72818/success\nKaggle Kernel: https://www.kaggle.com/aditya1702/adversarial-val-lgbm-xgb-model-lb-1-40\nhttps://www.kaggle.com/samratp/lightgbm-xgboost-catboost"},{"metadata":{},"cell_type":"markdown","source":"**LICENSE**"},{"metadata":{},"cell_type":"markdown","source":"Copyright 2019 FNU KARAN\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}